import os
import sys
from config import *
from torch import nn
from scipy.ndimage.filters import gaussian_filter1d
from torch.autograd import Variable
import torch
import numpy as np
import eval_utils as utils
import re
from tqdm import tqdm

def read_data(inputFile):
    filesAsStrings = []
    # Load entries from txt file:
    with open(inputFile, "r") as file:
        curFile = read_next_entry(file)
        while curFile != None:
            # Process the file
            curFile = read_next_entry(file)
            if curFile == None:
                break
            filesAsStrings.append(curFile)

    print("Finished processing files")

    # File is empty - return None
    if len(filesAsStrings) == 0:
        print("ERROR: No files found - check \"%s\" file" % (inputFile))
        return None
    
    return filesAsStrings

def read_next_entry(file):
    # with bininja processed txt file, we can now create embeddings
    curFile = []
    line = read_next_line(file)
    if line == None:
        if len(curFile) > 0:
            "".join(curFile)
            return curFile
        else:
            return None
    #check for end of file text (when we see lines like 12/55 new file starting)
    if re.search(r"^[0-9]*\/[0-9]*", line) != None:
        curFile.append(line)
    else:
        "".join(curFile)
    return curFile

def read_next_line(file):
    line = file.readline()
    if not line:
        return None
    return line

def get_entry_count(inputFile):
    #use grep to get lines with the format "i/# of entries" (indicating a new file)
    grepCall = os.subprocess.call(["grep", "-n", "^[0-9]*\/[0-9]*", inputFile, ">", "tmpOut.txt"])
    if grepCall.returncode != 0:
        print("ERROR: grep command failed - return code %d" % (grepCall.returncode))
        return None
    # Get the number of lines in the file
    with open("tmpOut.txt", "r") as file:
        lineCount = len(file.readlines())
    return lineCount

def setMaxEmbeddingSize(maxEmbeddingSize, inputFile):
    if inputFile == "./bininja_processed_malicous.txt":
        with open("maxEmbeddingSize_mal.txt", "w") as file:
            file.write(str(maxEmbeddingSize))
    else:
        with open("maxEmbeddingSize_ben.txt", "w") as file:
            file.write(str(maxEmbeddingSize))
 
def create_embeddings(model_path, vocab_path, inputFile, outDir):
    maxEmbeddingSize = 0    
    # Load the model
    palmtree = utils.UsableTransformer(model_path=model_path, vocab_path=vocab_path)
    #read in data from processed bininja file
    data = read_data(inputFile)
    embeddings = []
    if data == None:
        print("ERROR: No data found!")
        return
    for i in tqdm(range(len(data))):
        # Get the embeddings for the file
        embedding = palmtree.encode(data[i])
        if embedding.shape[0] > maxEmbeddingSize:
            maxEmbeddingSize = len(embedding)
        # Save the embeddings to a file
        np.save(outDir + "embedding_" + str(i) + ".npy", embeddings)
    # Save the max embedding size to a file; just makes things easier later on
    setMaxEmbeddingSize(maxEmbeddingSize, inputFile)

    

def main():
    model_path = "./palmtree/transformer.ep19"
    vocab_path = "./palmtree/vocab"
    sampleSwitch = sys.argv[0]

    # 1 for malicious, 0 for benign
    if sampleSwitch == 1:
        inputFile = "./bininja_processed_malicous.txt"
        outDir = "./embeddings/malicious/"
    else:
        inputFile = "./bininja_processed_benign.txt"
        outDir = "./embeddings/benign/"

    create_embeddings(model_path, vocab_path, inputFile, outDir)